import requests
import pandas as pd
import time

import requests

def search_places(keywords, rest_api_key, radius=1000, max_pages=3):
    """
    í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸ë¥¼ ë°›ì•„ì„œ, ê°€ê²Œ idì™€ ê°€ê²Œ ì´ë¦„ ë¦¬ìŠ¤íŠ¸ë¥¼ ìˆ˜ì§‘í•˜ëŠ” í•¨ìˆ˜
    """
    headers = {"Authorization": f"KakaoAK {rest_api_key}"}
    search_url = "https://dapi.kakao.com/v2/local/search/keyword.json"
    
    datas = []

    # ì²« ë²ˆì§¸ í‚¤ì›Œë“œë§Œ ì‚¬ìš©
    search_word = keywords[0] + " ë§›ì§‘"

    for page in range(1, max_pages + 1):
        params = {
            "query": search_word,
            "radius": radius,
            "page": page
        }
        
        response = requests.get(search_url, headers=headers, params=params, timeout=10)
        if response.status_code == 200:
            documents = response.json().get('documents', [])
            datas.extend(documents)
        else:
            print(f"âš ï¸ ê²€ìƒ‰ ì‹¤íŒ¨ (page {page}): {response.status_code}")
            break

    # ì—¬ê¸°ì„œ datasë¥¼ id_place_listë¡œ ë³€í™˜í•´ì¤˜ì•¼ í•´!!!
    id_place_list = [{'id': data['id'], 'placename': data['place_name']} for data in datas]

    return id_place_list


def scrape_reviews(id_place_list):
    """
    ê°€ê²Œ id ë¦¬ìŠ¤íŠ¸ë¥¼ ë°›ì•„ ë¦¬ë·°ë¥¼ ìˆ˜ì§‘í•˜ëŠ” í•¨ìˆ˜
    """
    COMMENT_URL = "https://place.map.kakao.com/m/commentlist/v/{}/{}?order=USEFUL&onlyPhotoComment=false"
    all_comment_df_list = []
    total_places = len(id_place_list)

    for idx, id_with_placename in enumerate(id_place_list, 1):
        all_comment = []
        comment_id = 0
        has_next = True

        while has_next:
            try:
                scrap_url = COMMENT_URL.format(id_with_placename['id'], comment_id)
                response = requests.get(scrap_url, timeout=10)
                json_response = response.json()
            except Exception as e:
                print(f"âš ï¸ {id_with_placename['placename']} ìš”ì²­ ì‹¤íŒ¨: {e}")
                break

            if 'comment' not in json_response:
                print(f"âš ï¸ {id_with_placename['placename']} ë¦¬ë·° ì—†ìŒ, ìŠ¤í‚µ")
                break

            comment_datas = json_response['comment']
            comment_list = comment_datas.get('list', [])

            for comment in comment_list:
                all_comment.append({
                    'ê°€ê²Œì´ë¦„': id_with_placename['placename'],
                    'ë¦¬ë·°ë‚´ìš©': comment.get('contents', '').strip(),
                    'ë¦¬ë·°ë³„ì ': comment.get('point', None)
                })

            has_next = comment_datas.get('hasNext', False)
            if has_next and comment_list:
                comment_id = comment_list[-1]['commentid']
            else:
                has_next = False

        if all_comment:
            df = pd.DataFrame(all_comment)
            all_comment_df_list.append(df)

        time.sleep(1)
        print(f"ê°€ê²Œ {total_places}ê°œ ì¤‘ {idx}ë²ˆì§¸ ì™„ë£Œ! 1ì´ˆ ì‰½ë‹ˆë‹¹ ğŸ’¤")

    if all_comment_df_list:
        return pd.concat(all_comment_df_list, ignore_index=True)
    else:
        return pd.DataFrame(columns=["ê°€ê²Œì´ë¦„", "ë¦¬ë·°ë‚´ìš©", "ë¦¬ë·°ë³„ì "])

def save_reviews_to_csv(final_df, filename):
    """
    ìµœì¢… ë¦¬ë·° ë°ì´í„°í”„ë ˆì„ì„ CSV íŒŒì¼ë¡œ ì €ì¥í•˜ëŠ” í•¨ìˆ˜
    """
    final_df = final_df[final_df['ë¦¬ë·°ë‚´ìš©'].str.strip() != '']
    final_df = final_df.dropna(subset=['ë¦¬ë·°ë‚´ìš©'])
    final_df.to_csv(filename, index=False, encoding='utf-8-sig')
    print(f"âœ… CSV ì €ì¥ ì™„ë£Œ: {filename}")

def main():
    REST_API_KEY = "668977541fc5a5fd5c0d271902df39c5"
    keyword = input("ì—­ ì´ë¦„ì„ ì…ë ¥í•˜ì„¸ìš” (ì˜ˆ: ì¶©ì •ë¡œì—­): ")

    print("ğŸ” ê°€ê²Œ ê²€ìƒ‰ ì¤‘...")
    id_place_list = search_places(keyword, REST_API_KEY)

    print("ğŸ“ ë¦¬ë·° í¬ë¡¤ë§ ì‹œì‘...")
    final_df = scrape_reviews(id_place_list)

    output_filename = f"{keyword}_ë¦¬ë·°ë°ì´í„°.csv"
    save_reviews_to_csv(final_df, output_filename)

if __name__ == "__main__":
    main()
