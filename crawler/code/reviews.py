import requests
import pandas as pd
import time

def search_places(keyword, rest_api_key, radius=1000, max_pages=4):
    headers = {"Authorization": f"KakaoAK {rest_api_key}"}
    search_url = "https://dapi.kakao.com/v2/local/search/keyword.json"
    datas = []

    search_word = keyword + " ë§›ì§‘"

    for page in range(1, max_pages + 1):
        params = {
            "query": search_word,
            "radius": radius,
            "page": page
        }
        response = requests.get(search_url, headers=headers, params=params, timeout=15)
        if response.status_code == 200:
            documents = response.json().get('documents', [])
            datas.extend(documents)
        else:
            print(f"âš ï¸ ê²€ìƒ‰ ì‹¤íŒ¨ (page {page}): {response.status_code}")
            break

    id_place_list = [{'id': data['id'], 'placename': data['place_name']} for data in datas]
    return id_place_list

def scrape_reviews(id_place_list):
    COMMENT_URL = "https://place.map.kakao.com/m/commentlist/v/{}/{}?order=USEFUL&onlyPhotoComment=false"
    all_comment = []

    for idx, id_with_placename in enumerate(id_place_list, 1):
        place_name = id_with_placename['placename']
        place_id = id_with_placename['id']
        comment_id = 0
        has_next = True

        while has_next:
            try:
                scrap_url = COMMENT_URL.format(place_id, comment_id)
                response = requests.get(scrap_url, timeout=15)
                json_response = response.json()
            except Exception as e:
                print(f"âš ï¸ {place_name} ìš”ì²­ ì‹¤íŒ¨: {e}")
                break

            if 'comment' not in json_response:
                print(f"âš ï¸ {place_name} ë¦¬ë·° ì—†ìŒ, ìŠ¤í‚µ")
                break

            comment_datas = json_response['comment']
            comment_list = comment_datas.get('list', [])

            for comment in comment_list:
                content = comment.get('contents', '').strip()
                point = comment.get('point', None)

                if content:  # ë¹ˆ ë¬¸ìì—´ ê±°ë¥´ê¸°
                    all_comment.append({
                        'ê°€ê²Œì´ë¦„': place_name,
                        'ë¦¬ë·°ë‚´ìš©': content,
                        'ë¦¬ë·°ë³„ì ': point
                    })

            has_next = comment_datas.get('hasNext', False)
            if has_next and comment_list:
                comment_id = comment_list[-1]['commentid']
            else:
                has_next = False

        time.sleep(1)
        print(f"{place_name} ({idx}/{len(id_place_list)}) ì™„ë£Œ! 1ì´ˆ ì‰¬ì–´ìš” ğŸ’¤")

    df = pd.DataFrame(all_comment)
    return df

def save_reviews_to_csv(final_df, filename):
    if final_df.empty:
        print("âš ï¸ ì €ì¥í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return
    final_df = final_df.dropna(subset=['ë¦¬ë·°ë‚´ìš©'])
    final_df = final_df[final_df['ë¦¬ë·°ë‚´ìš©'].str.strip() != '']
    final_df = final_df.sort_values(by=['ê°€ê²Œì´ë¦„']).reset_index(drop=True)
    final_df.to_csv(filename, index=False, encoding='utf-8-sig')
    print(f"âœ… CSV ì €ì¥ ì™„ë£Œ: {filename}")

def main():
    REST_API_KEY = "668977541fc5a5fd5c0d271902df39c5"
    keyword = input("ì—­ ì´ë¦„ì„ ì…ë ¥í•˜ì„¸ìš” (ì˜ˆ: ì¶©ì •ë¡œì—­): ").strip()

    print("ğŸ” ê°€ê²Œ ê²€ìƒ‰ ì¤‘...")
    id_place_list = search_places(keyword, REST_API_KEY)

    print("ğŸ“ ë¦¬ë·° í¬ë¡¤ë§ ì‹œì‘...")
    final_df = scrape_reviews(id_place_list)

    output_filename = f"{keyword}_ë¦¬ë·°ë°ì´í„°.csv"
    save_reviews_to_csv(final_df, output_filename)

if __name__ == "__main__":
    main()