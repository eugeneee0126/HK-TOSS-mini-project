# reviews.py
# Description: 'stores.csv'ì—ì„œ ê°€ê²Œ ì´ë¦„ ëª©ë¡ì„ ë¶ˆëŸ¬ì™€, ê° ê°€ê²Œ ì´ë¦„ìœ¼ë¡œ ì¹´ì¹´ì˜¤ë§µ APIë¥¼ í˜¸ì¶œí•´ place_idë¥¼ ì°¾ê³ ,
#              í•´ë‹¹ ê°€ê²Œì˜ ë¦¬ë·°ë¥¼ ìµœëŒ€ 50ê°œê¹Œì§€ ìˆ˜ì§‘í•˜ì—¬ 'reviews.csv'ë¡œ ì €ì¥í•˜ëŠ” ìŠ¤í¬ë¦½íŠ¸ì…ë‹ˆë‹¤.
# Author: í†µí•©ë²„ì „
# Date: 2025.04.29
# Requirements: requests, pandas


import requests
import pandas as pd
import time
import sys
import os

def search_place_id(store_name, rest_api_key):
    """ê°€ê²Œ ì´ë¦„ìœ¼ë¡œ place_idë¥¼ ê²€ìƒ‰ (ê°€ì¥ ì¼ì¹˜ìœ¨ ë†’ì€ ê²°ê³¼ 1ê±´)"""
    headers = {"Authorization": f"KakaoAK {rest_api_key}"}
    url = "https://dapi.kakao.com/v2/local/search/keyword.json"
    params = {"query": store_name, "size": 1}

    response = requests.get(url, headers=headers, params=params, timeout=10)
    if response.status_code == 200:
        documents = response.json().get("documents", [])
        if documents:
            return documents[0]["id"]
    return None

def scrape_reviews_by_storelist(store_names, rest_api_key):
    COMMENT_URL = "https://place.map.kakao.com/m/commentlist/v/{}/{}?order=USEFUL&onlyPhotoComment=false"
    all_comment = []

    for idx, store_name in enumerate(store_names, 1):
        place_id = search_place_id(store_name, rest_api_key)
        if not place_id:
            print(f"âŒ {store_name}ì˜ place_idë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            continue

        comment_id = 0
        has_next = True
        max_reviews = 50
        review_count = 0

        while has_next:
            try:
                scrap_url = COMMENT_URL.format(place_id, comment_id)
                response = requests.get(scrap_url, timeout=10)
                data = response.json()
            except Exception as e:
                print(f"âš ï¸ {store_name} ìš”ì²­ ì‹¤íŒ¨: {e}")
                break

            if 'comment' not in data:
                print(f"âš ï¸ {store_name} ë¦¬ë·° ì—†ìŒ, ìŠ¤í‚µ")
                break

            comment_datas = data['comment']
            comment_list = comment_datas.get('list', [])

            for comment in comment_list:
                content = comment.get('contents', '').strip()
                point = comment.get('point', None)

                if content:
                    all_comment.append({
                        'ê°€ê²Œì´ë¦„': store_name,
                        'ë¦¬ë·°ë‚´ìš©': content,
                        'ë¦¬ë·°ë³„ì ': point
                    })
                    review_count += 1
                    if review_count >= max_reviews:
                        has_next = False
                        break

            has_next = comment_datas.get('hasNext', False)
            if has_next and comment_list:
                comment_id = comment_list[-1]['commentid']
            else:
                has_next = False

        time.sleep(1)
        print(f"{store_name} ({idx}/{len(store_names)}) ì™„ë£Œ! 1ì´ˆ ì‰¬ì–´ìš” ğŸ’¤")

    return pd.DataFrame(all_comment)

def save_reviews_to_csv(final_df, filename):
    if final_df.empty:
        print("âš ï¸ ì €ì¥í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return
    final_df = final_df.dropna(subset=['ë¦¬ë·°ë‚´ìš©'])
    final_df = final_df[final_df['ë¦¬ë·°ë‚´ìš©'].str.strip() != '']
    final_df = final_df.sort_values(by=['ê°€ê²Œì´ë¦„']).reset_index(drop=True)
    final_df.to_csv(filename, index=False, encoding='utf-8-sig')
    print(f"âœ… CSV ì €ì¥ ì™„ë£Œ: {filename}")

def main():
    if len(sys.argv) < 2:
        print("âŒ ì—­ ì´ë¦„ì´ ì „ë‹¬ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        return

    REST_API_KEY = "668977541fc5a5fd5c0d271902df39c5"
    stores_csv_path = "data/stores.csv"

    if not os.path.exists(stores_csv_path):
        print("âŒ stores.csv íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
        return

    store_df = pd.read_csv(stores_csv_path)
    store_names = store_df["store_name"].dropna().unique().tolist()

    print("ğŸ“ ë¦¬ë·° í¬ë¡¤ë§ ì‹œì‘...")
    final_df = scrape_reviews_by_storelist(store_names, REST_API_KEY)

    save_reviews_to_csv(final_df, "data/reviews.csv")

if __name__ == "__main__":
    main()
